<div align="left">
  <h1> 9. Pandas Cheatsheet - Numerical Variable Operations

  ## Numerical Variable Operations

### 1. Build in Datasets

```shell
tips
iris
penguins
flights
diamonds
titanic
exercise
mpg
planets
anagrams
anscombe
attention
brain_networks
car_crashes
dots
dowjones
fmri
geyser
glue
healthexp
seaice
taxis
```
### 2. Load the dataset

```py
import seaborn as sns

# Load the tips dataset
tips = sns.load_dataset('tips')
```

### 3. Arithmetic Operations ( df.add(s) , df.sub(s) , df.mul(s) , df.div(s) , df.floordiv(s) , df.mod(s) , df.pow(s) )
It returns column with the label col as Series.

```py
import seaborn as sns
import pandas as pd
import numpy as np # For potential NaN if operations result in it, though not directly used in core ops

# Load the iris dataset from seaborn
iris = sns.load_dataset('iris')

# Display the first 5 rows of the DataFrame
print("Original Iris DataFrame (first 5 rows):\n", iris.head(5))

# --- Start of numerical operations demonstration ---

# For demonstration, we will pick a subset of the numerical columns
# to keep the output concise and focused.
numerical_df = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]

print("\nNumerical DataFrame subset (first 5 rows):\n", numerical_df.head(5))

# Define a scalar 's' for element-wise operations
s_scalar = 2.0

# Define a Series 's_series' for operations along an axis
# This Series will have an index that aligns with the DataFrame's index.
# For simplicity, let's create a Series based on 'sepal_length'
s_series = numerical_df['sepal_length'] / 10 # Create a small series for demonstration


print(f"\n--- Operations using scalar s = {s_scalar} ---")

# 1. df.add(s) - Element-wise addition
numerical_df['sepal_length_plus_2'] = numerical_df['sepal_length'].add(s_scalar)
print("\nAfter adding 2.0 to 'sepal_length':\n", numerical_df[['sepal_length', 'sepal_length_plus_2']].head(5))

# 2. df.sub(s) - Element-wise subtraction
numerical_df['sepal_width_minus_2'] = numerical_df['sepal_width'].sub(s_scalar)
print("\nAfter subtracting 2.0 from 'sepal_width':\n", numerical_df[['sepal_width', 'sepal_width_minus_2']].head(5))

# 3. df.mul(s) - Element-wise multiplication
numerical_df['petal_length_times_2'] = numerical_df['petal_length'].mul(s_scalar)
print("\nAfter multiplying 'petal_length' by 2.0:\n", numerical_df[['petal_length', 'petal_length_times_2']].head(5))

# 4. df.div(s) - Element-wise division (float division)
numerical_df['petal_width_div_2'] = numerical_df['petal_width'].div(s_scalar)
print("\nAfter dividing 'petal_width' by 2.0:\n", numerical_df[['petal_width', 'petal_width_div_2']].head(5))

# 5. df.floordiv(s) - Element-wise floor division (integer division)
# Note: For float inputs, it still returns float results, but floors the division.
numerical_df['sepal_length_floordiv_2'] = numerical_df['sepal_length'].floordiv(s_scalar)
print("\nAfter floor dividing 'sepal_length' by 2.0:\n", numerical_df[['sepal_length', 'sepal_length_floordiv_2']].head(5))

# 6. df.mod(s) - Element-wise modulo (remainder)
numerical_df['sepal_width_mod_2'] = numerical_df['sepal_width'].mod(s_scalar)
print("\nAfter modulo 2.0 on 'sepal_width':\n", numerical_df[['sepal_width', 'sepal_width_mod_2']].head(5))

# 7. df.pow(s) - Element-wise exponentiation
numerical_df['petal_length_pow_2'] = numerical_df['petal_length'].pow(s_scalar)
print("\nAfter raising 'petal_length' to the power of 2.0:\n", numerical_df[['petal_length', 'petal_length_pow_2']].head(5))


print(f"\n--- Operations using Series 's_series' (first 5 values): ---")
print(s_series.head(5))

# Example of an operation using a Series (index-aligned)
numerical_df['sepal_length_plus_series'] = numerical_df['sepal_length'].add(s_series)
print("\nAfter adding 's_series' to 'sepal_length' (element-wise):\n",
      numerical_df[['sepal_length', 'sepal_length_plus_series', s_series.name]].head(5))
# Note how the 's_series' value (e.g., 0.51 for row 0) is added to sepal_length (5.1).

```

![Screenshot 2025-06-14 231639](https://github.com/user-attachments/assets/0b2c8d72-a7bc-48e9-a3e2-a3774aaa571a)



### 4. Comparison Operations ( df.eq(s) , df.ne(s) , df.le(s) , df.lt(s) , df.ge(s) , df.gt(s) )

```py
import seaborn as sns
import pandas as pd
import numpy as np # Not directly used for operations but generally useful

# Load the iris dataset from seaborn
iris = sns.load_dataset('iris')

# Display the first 5 rows of the DataFrame
print("Original Iris DataFrame (first 5 rows):\n", iris.head(5))

# --- Start of numerical comparison operations demonstration ---

# For demonstration, we will pick a subset of the numerical columns
numerical_df = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].copy()
# .copy() to avoid SettingWithCopyWarning when adding new columns

print("\nNumerical DataFrame subset (first 5 rows):\n", numerical_df.head(5))

# Define a scalar 's' for element-wise comparison
s_compare = 5.0

print(f"\n--- Comparison Operations using scalar s = {s_compare} on 'sepal_length' ---")

# 1. df.eq(s) - Element-wise Equal to (==)
numerical_df['sepal_length_eq_5'] = numerical_df['sepal_length'].eq(s_compare)
print("\n'sepal_length' == 5.0 (first 5 rows):\n",
      numerical_df[['sepal_length', 'sepal_length_eq_5']].head(5))

# 2. df.ne(s) - Element-wise Not Equal to (!=)
numerical_df['sepal_length_ne_5'] = numerical_df['sepal_length'].ne(s_compare)
print("\n'sepal_length' != 5.0 (first 5 rows):\n",
      numerical_df[['sepal_length', 'sepal_length_ne_5']].head(5))

# 3. df.le(s) - Element-wise Less than or Equal to (<=)
numerical_df['sepal_length_le_5'] = numerical_df['sepal_length'].le(s_compare)
print("\n'sepal_length' <= 5.0 (first 5 rows):\n",
      numerical_df[['sepal_length', 'sepal_length_le_5']].head(5))

# 4. df.lt(s) - Element-wise Less than (<)
numerical_df['sepal_length_lt_5'] = numerical_df['sepal_length'].lt(s_compare)
print("\n'sepal_length' < 5.0 (first 5 rows):\n",
      numerical_df[['sepal_length', 'sepal_length_lt_5']].head(5))

# 5. df.ge(s) - Element-wise Greater than or Equal to (>=)
numerical_df['sepal_length_ge_5'] = numerical_df['sepal_length'].ge(s_compare)
print("\n'sepal_length' >= 5.0 (first 5 rows):\n",
      numerical_df[['sepal_length', 'sepal_length_ge_5']].head(5))

# 6. df.gt(s) - Element-wise Greater than (>)
numerical_df['sepal_length_gt_5'] = numerical_df['sepal_length'].gt(s_compare)
print("\n'sepal_length' > 5.0 (first 5 rows):\n",
      numerical_df[['sepal_length', 'sepal_length_gt_5']].head(5))

# You can also apply these comparisons to the entire DataFrame subset to get a boolean DataFrame
print(f"\n--- Applying 'sepal_width' > {s_compare} to the entire numerical_df (returns boolean DataFrame) ---")
boolean_df_gt_s = numerical_df.gt(s_compare)
print(boolean_df_gt_s.head(5))

```


![Screenshot 2025-06-14 231247](https://github.com/user-attachments/assets/65181e4e-e71f-4912-976d-e2d3e1e41248)


### 5. Ordered Numerical Data - Largest N

```py
import pandas as pd
import numpy as np # For creating sample data

# --- Create a custom sample DataFrame ---
# This DataFrame represents hypothetical student scores across different subjects
# and includes some duplicate scores to demonstrate 'keep' parameter variations.
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
    'Math_Score': [85, 92, 78, 95, 88, 70, 92, 85, 90, 75, 95, 88, 78, 80, 92],
    'Physics_Score': [70, 88, 95, 75, 90, 80, 72, 88, 85, 95, 70, 90, 80, 75, 92],
    'Attendance': [90, 95, 80, 98, 92, 85, 91, 90, 88, 93, 98, 92, 85, 80, 91]
}
df_students = pd.DataFrame(data)

# Display the entire sample DataFrame
print("Original Sample Student Scores DataFrame:\n", df_students)
print("\n--- Basic Numerical Operations with nlargest() on Sample Data ---")

# --- 1. Basic usage: nlargest by a single column ---
# Find the top 5 students with the largest 'Math_Score'
print("\nTop 5 students by 'Math_Score' (default keep='first'):")
nlargest_math = df_students.nlargest(n=5, columns='Math_Score')
print(nlargest_math)

# --- 2. nlargest by a different single column ---
# Find the top 3 students with the largest 'Physics_Score'
print("\nTop 3 students by 'Physics_Score':")
nlargest_physics = df_students.nlargest(n=3, columns='Physics_Score')
print(nlargest_physics)

# --- 3. nlargest by multiple columns ---
# Find the top 5 students based on 'Math_Score' first, then 'Physics_Score' if Math_Score is tied
print("\nTop 5 students by 'Math_Score' then 'Physics_Score':")
nlargest_multi_col = df_students.nlargest(n=5, columns=['Math_Score', 'Physics_Score'])
print(nlargest_multi_col)

# --- 4. Handling duplicates with 'keep' parameter ---

# We have duplicates in 'Math_Score' (e.g., 95, 92, 88, 85, 78) to demonstrate 'keep'
# Let's find the top 5 'Math_Score' to illustrate the 'keep' parameter with actual ties.

# a) keep='first' (default behavior): Returns the first occurrence in case of ties
print("\nnlargest(n=5, columns='Math_Score', keep='first'):")
nlargest_keep_first = df_students.nlargest(n=5, columns='Math_Score', keep='first')
print(nlargest_keep_first)
# Observe if there are ties, which Student_ID is returned (the one that appeared earlier in the original df).

# b) keep='last': Returns the last occurrence in case of ties
print("\nnlargest(n=5, columns='Math_Score', keep='last'):")
nlargest_keep_last = df_students.nlargest(n=5, columns='Math_Score', keep='last')
print(nlargest_keep_last)
# Observe if there are ties, which Student_ID is returned (the one that appeared later in the original df).

# c) keep='all': Returns all occurrences in case of ties, potentially returning more than 'n' rows
print("\nnlargest(n=5, columns='Math_Score', keep='all'):")
nlargest_keep_all = df_students.nlargest(n=5, columns='Math_Score', keep='all')
print(nlargest_keep_all)
# Note how this can result in more than 5 rows if there are multiple students
# sharing the 5th (or higher) largest score.

```

### 6. Ordered Numerical Data - Smallest N

```py
import pandas as pd
import numpy as np # For creating sample data

# --- Create a custom sample DataFrame ---
# This DataFrame represents hypothetical student scores across different subjects
# and includes some duplicate scores to demonstrate 'keep' parameter variations.
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
    'Math_Score': [85, 92, 78, 95, 88, 70, 92, 85, 90, 75, 95, 88, 78, 80, 92],
    'Physics_Score': [70, 88, 95, 75, 90, 80, 72, 88, 85, 95, 70, 90, 80, 75, 92],
    'Attendance': [90, 95, 80, 98, 92, 85, 91, 90, 88, 93, 98, 92, 85, 80, 91]
}
df_students = pd.DataFrame(data)

# Display the entire sample DataFrame
print("Original Sample Student Scores DataFrame:\n", df_students)
print("\n--- Basic Numerical Operations with nsmallest() on Sample Data ---")

# --- 1. Basic usage: nsmallest by a single column ---
# Find the bottom 5 students with the smallest 'Math_Score'
print("\nBottom 5 students by 'Math_Score' (default keep='first'):")
nsmallest_math = df_students.nsmallest(n=5, columns='Math_Score')
print(nsmallest_math)

# --- 2. nsmallest by a different single column ---
# Find the bottom 3 students with the smallest 'Physics_Score'
print("\nBottom 3 students by 'Physics_Score':")
nsmallest_physics = df_students.nsmallest(n=3, columns='Physics_Score')
print(nsmallest_physics)

# --- 3. nsmallest by multiple columns ---
# Find the bottom 5 students based on 'Math_Score' first, then 'Physics_Score' if Math_Score is tied
print("\nBottom 5 students by 'Math_Score' then 'Physics_Score':")
nsmallest_multi_col = df_students.nsmallest(n=5, columns=['Math_Score', 'Physics_Score'])
print(nsmallest_multi_col)

# --- 4. Handling duplicates with 'keep' parameter ---

# We have duplicates in 'Math_Score' (e.g., 70, 75, 78, 80, 85) to demonstrate 'keep'
# Let's find the bottom 5 'Math_Score' to illustrate the 'keep' parameter with actual ties.

# a) keep='first' (default behavior): Returns the first occurrence in case of ties
print("\nnsmallest(n=5, columns='Math_Score', keep='first'):")
nsmallest_keep_first = df_students.nsmallest(n=5, columns='Math_Score', keep='first')
print(nsmallest_keep_first)
# Observe if there are ties, which Student_ID is returned (the one that appeared earlier in the original df).

# b) keep='last': Returns the last occurrence in case of ties
print("\nnsmallest(n=5, columns='Math_Score', keep='last'):")
nsmallest_keep_last = df_students.nsmallest(n=5, columns='Math_Score', keep='last')
print(nsmallest_keep_last)
# Observe if there are ties, which Student_ID is returned (the one that appeared later in the original df).

# c) keep='all': Returns all occurrences in case of ties, potentially returning more than 'n' rows
print("\nnsmallest(n=5, columns='Math_Score', keep='all'):")
nsmallest_keep_all = df_students.nsmallest(n=5, columns='Math_Score', keep='all')
print(nsmallest_keep_all)
# Note how this can result in more than 5 rows if there are multiple students
# sharing the 5th (or lower) smallest score.

```

### 7. Ordered Numerical Data - Rank

```py
import pandas as pd
import numpy as np # For creating sample data and NaNs

# --- Create a custom sample DataFrame ---
# This DataFrame represents hypothetical student scores and includes
# duplicate values and missing values (NaN) to demonstrate various rank options.
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Score': [85, 90, 75, 90, 85, 70, np.nan, 80, 95, 75],
    'Grade': ['B', 'A', 'C', 'A', 'B', 'D', np.nan, 'C', 'A+', 'C']
}
df_students = pd.DataFrame(data)

# Display the entire sample DataFrame
print("Original Sample Student Scores DataFrame:\n", df_students)
print("\n--- Basic Numerical Operations with df.rank() on Sample Data ---")

# --- 1. Method variations (default is 'average') ---
print("\n--- Method Variations for Ranking 'Score' ---")

# a) method='average' (default): Assigns the average rank to tied values.
df_students['Rank_Average'] = df_students['Score'].rank(method='average')
print("\nRank with method='average':\n", df_students[['Score', 'Rank_Average']].sort_values('Score'))

# b) method='min': Assigns the minimum rank in the group of tied values.
df_students['Rank_Min'] = df_students['Score'].rank(method='min')
print("\nRank with method='min':\n", df_students[['Score', 'Rank_Min']].sort_values('Score'))

# c) method='max': Assigns the maximum rank in the group of tied values.
df_students['Rank_Max'] = df_students['Score'].rank(method='max')
print("\nRank with method='max':\n", df_students[['Score', 'Rank_Max']].sort_values('Score'))

# d) method='first': Assigns ranks in the order of appearance for tied values.
df_students['Rank_First'] = df_students['Score'].rank(method='first')
print("\nRank with method='first':\n", df_students[['Score', 'Rank_First']].sort_values('Score'))

# e) method='dense': Assigns ranks without gaps in case of ties.
df_students['Rank_Dense'] = df_students['Score'].rank(method='dense')
print("\nRank with method='dense':\n", df_students[['Score', 'Rank_Dense']].sort_values('Score'))


# --- 2. Ascending / Descending ---
print("\n--- Ascending / Descending for Ranking 'Score' ---")

# a) ascending=True (default): Smallest value gets rank 1.
df_students['Rank_Ascending'] = df_students['Score'].rank(ascending=True)
print("\nRank with ascending=True (default method='average'):\n", df_students[['Score', 'Rank_Ascending']].sort_values('Score'))

# b) ascending=False: Largest value gets rank 1.
df_students['Rank_Descending'] = df_students['Score'].rank(ascending=False)
print("\nRank with ascending=False (default method='average'):\n", df_students[['Score', 'Rank_Descending']].sort_values('Score', ascending=False))


# --- 3. Percentile Ranking ---
print("\n--- Percentile Ranking for 'Score' ---")

# pct=True: Ranks values as percentages (0 to 1) relative to the total number of non-NaN values.
df_students['Rank_Percentile'] = df_students['Score'].rank(pct=True)
print("\nRank as Percentile (method='average'):\n", df_students[['Score', 'Rank_Percentile']].sort_values('Score'))


# --- 4. Handling NaN (na_option) ---
print("\n--- Handling NaN Values for Ranking 'Score' ---")

# Create a fresh copy to demonstrate NaN handling clearly
df_nan_demo = pd.DataFrame(data)

# a) na_option='keep' (default): NaN values are assigned a NaN rank.
df_nan_demo['Rank_NaN_Keep'] = df_nan_demo['Score'].rank(na_option='keep')
print("\nRank with na_option='keep':\n", df_nan_demo[['Score', 'Rank_NaN_Keep']].sort_values('Score'))

# b) na_option='top': NaN values are ranked as the smallest values (assigned rank 1, or highest rank if descending).
df_nan_demo['Rank_NaN_Top'] = df_nan_demo['Score'].rank(na_option='top')
print("\nRank with na_option='top':\n", df_nan_demo[['Score', 'Rank_NaN_Top']].sort_values('Score'))

# c) na_option='bottom': NaN values are ranked as the largest values (assigned the highest rank).
df_nan_demo['Rank_NaN_Bottom'] = df_nan_demo['Score'].rank(na_option='bottom')
print("\nRank with na_option='bottom':\n", df_nan_demo[['Score', 'Rank_NaN_Bottom']].sort_values('Score'))


```

![Screenshot 2025-06-14 231357](https://github.com/user-attachments/assets/b62fb3d1-1c40-4aa2-b5af-5d61f50906a4)


### 8. Binning / Discretize and Clipping Data - Discretize Data( pd.cut() )

```py
import pandas as pd
import numpy as np # For creating sample data and NaNs

# --- Create a custom sample DataFrame ---
# This DataFrame represents hypothetical student scores, suitable for binning.
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
    'Score': [85, 92, 78, 95, 88, 70, 62, 80, 98, 72, 55, 68, 83, 79, 91],
    'Study_Hours': [5, 10, 3, 12, 7, 2, 1, 6, 15, 4, 1, 3, 7, 5, 9]
}
df_students = pd.DataFrame(data)

# Display the entire sample DataFrame
print("Original Sample Student Scores DataFrame:\n", df_students)
print("\n--- Binning Numerical Data with pd.cut() ---")

# --- 1. Simple bins (integer number) ---
# Divides the 'Score' column into 4 equally sized bins.
df_students['Score_Bin_4'] = pd.cut(df_students['Score'], bins=4)
print("\n1. 'Score' binned into 4 equal-width bins:\n", df_students[['Score', 'Score_Bin_4']].head(10))
print("Value counts for 'Score_Bin_4':\n", df_students['Score_Bin_4'].value_counts().sort_index())


# --- 2. Binning with precision ---
# By default, cut tries to make bin edges pretty. You can control precision.
# Here, bins are still 4, but the representation might show more precision if needed.
# The default precision is usually sufficient, but we can illustrate by observing the default.
print("\n2. 'Score' binned into 4 bins (default precision, often good enough):")
df_students['Score_Bin_Precision'] = pd.cut(df_students['Score'], bins=4, precision=1) # Precision argument
print(df_students[['Score', 'Score_Bin_Precision']].head(10))
print("Value counts for 'Score_Bin_Precision':\n", df_students['Score_Bin_Precision'].value_counts().sort_index())


# --- 3. Binning with labels (explicit list of labels) ---
# Divides 'Score' into 4 bins and assigns custom labels to them.
labels_4_bins = ['Low', 'Moderate', 'High', 'Extreme']
df_students['Score_Bin_Labels'] = pd.cut(df_students['Score'], bins=4, labels=labels_4_bins)
print("\n3. 'Score' binned into 4 bins with explicit labels:\n", df_students[['Score', 'Score_Bin_Labels']].head(10))
print("Value counts for 'Score_Bin_Labels':\n", df_students['Score_Bin_Labels'].value_counts().sort_index())


# --- 4. Custom bins with labels ---
# Define explicit bin edges and assign custom labels to each bin.
custom_bins = [0, 60, 70, 80, 90, 100] # Defines ranges: 0-60, 60-70, 70-80, 80-90, 90-100
custom_labels = ['Fail', 'Poor', 'Average', 'Good', 'Excellent']
df_students['Score_Grades'] = pd.cut(df_students['Score'], bins=custom_bins, labels=custom_labels, right=False) # right=False makes intervals [start, end)
print("\n4. 'Score' binned with custom bins and labels (right=False):\n", df_students[['Score', 'Score_Grades']].head(10))
print("Value counts for 'Score_Grades':\n", df_students['Score_Grades'].value_counts().sort_index())


# --- 5. Bin edge manipulation (right=False) ---
# Default is right=True (intervals are (a, b]), meaning 'b' is included in the bin.
# right=False makes intervals [a, b), meaning 'a' is included.
custom_bins_alt = [50, 70, 80, 90, 100]
df_students['Score_Bin_RightFalse'] = pd.cut(df_students['Score'], bins=custom_bins_alt, right=False, labels=['50-69', '70-79', '80-89', '90-100'])
print("\n5. 'Score' binned with right=False (intervals are [start, end) ):\n", df_students[['Score', 'Score_Bin_RightFalse']].head(10))
print("Value counts for 'Score_Bin_RightFalse':\n", df_students['Score_Bin_RightFalse'].value_counts().sort_index())


# --- 6. Binning with partial range (include_lowest=True) ---
# Ensures that the lowest value in the data is included in the first bin,
# even if the bin definition's lower bound would otherwise exclude it (e.g., for (a,b] type bins).
# Useful when the data's minimum falls exactly on the lower edge of the first bin.
# Let's consider 'Study_Hours' for this. Its min is 1.
bins_study = [0, 5, 10, 15] # (0, 5], (5, 10], (10, 15]
df_students['Hours_Bin_IncludeLowest'] = pd.cut(df_students['Study_Hours'], bins=bins_study, include_lowest=True, labels=['Low', 'Medium', 'High'])
print("\n6. 'Study_Hours' binned with include_lowest=True:\n", df_students[['Study_Hours', 'Hours_Bin_IncludeLowest']].head(10))
print("Value counts for 'Hours_Bin_IncludeLowest':\n", df_students['Hours_Bin_IncludeLowest'].value_counts().sort_index())


# --- 7. Binning with interval definition ---
# This is the default output when `labels` are not explicitly provided and `bins` is an integer.
# It returns a Categorical Series where categories are Interval objects.
df_students['Score_Interval_Objects'] = pd.cut(df_students['Score'], bins=5)
print("\n7. 'Score' binned into 5 intervals (default output without explicit labels):\n", df_students[['Score', 'Score_Interval_Objects']].head(10))
print("Type of bin values:", type(df_students['Score_Interval_Objects'].iloc[0]))
print("Value counts for 'Score_Interval_Objects':\n", df_students['Score_Interval_Objects'].value_counts().sort_index())

```

### 9. Binning / Discretize and Clipping Data - Discretize Data( pd.qcut() )

```py
import pandas as pd
import numpy as np # For creating sample data

# --- Create a custom sample DataFrame ---
# This DataFrame represents hypothetical student scores, suitable for quantile-based binning.
# It includes some duplicate scores to demonstrate how 'duplicates' parameter works.
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
    'Score': [85, 92, 78, 95, 88, 70, 62, 80, 98, 72, 55, 68, 83, 79, 91, 70, 85, 92, 78, 65],
    'Test_Time_Minutes': [45, 30, 60, 25, 40, 55, 65, 50, 20, 52, 70, 58, 48, 51, 35, 54, 42, 33, 61, 63]
}
df_students = pd.DataFrame(data)

# Display the entire sample DataFrame
print("Original Sample Student Scores DataFrame:\n", df_students)
print("\n--- Quantile-based Binning with pd.qcut() ---")

# --- 1. Simple q-bins (q=4 for quartiles) ---
# Divides the 'Score' column into 4 bins, where each bin has approximately
# the same number of observations (equal frequency bins).
df_students['Score_Quartiles'] = pd.qcut(df_students['Score'], q=4)
print("\n1. 'Score' binned into 4 quartiles:\n", df_students[['Score', 'Score_Quartiles']].head(10))
print("Value counts for 'Score_Quartiles' (should be roughly equal distribution):\n",
      df_students['Score_Quartiles'].value_counts().sort_index())


# --- 2. Binning with labels ---
# Assign custom labels to the quantile bins.
quantile_labels = ['Bottom Quartile', 'Second Quartile', 'Third Quartile', 'Top Quartile']
df_students['Score_Quartile_Labels'] = pd.qcut(df_students['Score'], q=4, labels=quantile_labels)
print("\n2. 'Score' binned into 4 quartiles with explicit labels:\n", df_students[['Score', 'Score_Quartile_Labels']].head(10))
print("Value counts for 'Score_Quartile_Labels':\n",
      df_students['Score_Quartile_Labels'].value_counts().sort_index())


# --- 3. Handling duplicates ('duplicates' parameter) ---
# The 'Score' column has duplicate values (e.g., 70, 78, 85, 92, 95).
# qcut by default raises an error if duplicate bin edges are generated.
# 'duplicates='drop'' allows duplicate bin edges to be dropped.

# Let's try to qcut 'Test_Time_Minutes' into many quantiles, potentially creating duplicates
# (This is more likely with smaller ranges or many duplicates in the data)
# For 'Score', with only 20 unique values, q=20 could cause issues.
# q=5 for 'Test_Time_Minutes' as an example:
print("\n3. 'Test_Time_Minutes' binned into 5 quantiles, handling duplicates (if any):")
try:
    df_students['Test_Time_Quintiles'] = pd.qcut(df_students['Test_Time_Minutes'], q=5, duplicates='drop')
    print(df_students[['Test_Time_Minutes', 'Test_Time_Quintiles']].head(10))
    print("Value counts for 'Test_Time_Quintiles':\n", df_students['Test_Time_Quintiles'].value_counts().sort_index())
except ValueError as e:
    print(f"Could not bin with default 'duplicates' due to error: {e}")
    print("Trying with duplicates='drop' (already applied above in the try block for Test_Time_Quintiles)")
    # If the error was about duplicate edges, 'duplicates='drop'' should handle it.
    # If using 'Score' with a very high 'q' and many duplicates, 'drop' is essential.


# --- 4. Binning with custom quantiles ---
# Instead of an integer 'q', provide a list of explicit quantile values (0 to 1).
# Example: 10th percentile, 50th percentile (median), 90th percentile
custom_quantiles = [0, 0.1, 0.5, 0.9, 1.0] # Must include 0 and 1
custom_quantile_labels = ['Bottom 10%', '10-50%', '50-90%', 'Top 10%']
df_students['Score_Custom_Quantiles'] = pd.qcut(df_students['Score'], q=custom_quantiles, labels=custom_quantile_labels)
print("\n4. 'Score' binned with custom quantile points and labels:\n", df_students[['Score', 'Score_Custom_Quantiles']].head(10))
print("Value counts for 'Score_Custom_Quantiles':\n",
      df_students['Score_Custom_Quantiles'].value_counts().sort_index())

```

### 10. Binning / Discretize and Clipping Data - Clip Data

```py
import pandas as pd
import numpy as np # For creating sample data and NaNs

# --- Create a custom sample DataFrame ---
# This DataFrame represents hypothetical monthly sales figures,
# which might contain outliers (very high sales) or erroneous negative values (e.g., returns processed incorrectly).
data = {
    'Month': pd.to_datetime(['2024-01-01', '2024-02-01', '2024-03-01', '2024-04-01', '2024-05-01',
                             '2024-06-01', '2024-07-01', '2024-08-01', '2024-09-01', '2024-10-01']),
    'Sales_USD': [15000, 18000, -500, 22000, 19000,
                  16000, 25000, 17500, -100, 30000], # Contains an outlier (30000) and negatives
    'Units_Sold': [150, 180, 5, 220, 190,
                   160, 250, 175, 10, 300]
}
df_sales = pd.DataFrame(data)

# Display the entire sample DataFrame
print("Original Sample Sales DataFrame:\n", df_sales)
print("\n--- Basic Numerical Operations with df.clip() ---")

# --- 1. Fit outliers to a defined upper threshold ---
# Objective: Cap all sales values that are above a certain threshold (e.g., $22,000)
# This is useful to prevent extreme values from disproportionately affecting analysis or models.
upper_threshold = 22000
df_sales['Sales_Capped_Upper'] = df_sales['Sales_USD'].clip(upper=upper_threshold)

print(f"\n1. 'Sales_USD' after clipping outliers to upper threshold of ${upper_threshold:,.0f}:")
print(df_sales[['Sales_USD', 'Sales_Capped_Upper']].sort_values(by='Sales_USD', ascending=False).head(5))
# Observe that 25000 and 30000 are now capped at 22000.

# --- 2. Convert negative values to zero ---
# Objective: Ensure all sales values are non-negative.
# Negative sales could represent returns, but for many analyses, a floor of zero is desired.
lower_threshold = 0
df_sales['Sales_Non_Negative'] = df_sales['Sales_USD'].clip(lower=lower_threshold)

print(f"\n2. 'Sales_USD' after converting negative values to zero (lower threshold of {lower_threshold}):")
print(df_sales[['Sales_USD', 'Sales_Non_Negative']].sort_values(by='Sales_USD').head(5))
# Observe that -500 and -100 are now 0.

# --- Combining both: Fit values within a min-max range ---
# This is a common use case for clip: setting both a lower and an upper bound.
min_sales_value = 500 # Example: Minimum plausible sales per month
max_sales_value = 20000 # Example: Maximum expected sales, capping high outliers
df_sales['Sales_Cleaned_Range'] = df_sales['Sales_USD'].clip(lower=min_sales_value, upper=max_sales_value)

print(f"\nCombined: 'Sales_USD' after clipping to range [{min_sales_value}, {max_sales_value}]:")
print(df_sales[['Sales_USD', 'Sales_Cleaned_Range']].sort_values(by='Sales_USD').head(10))
# Observe how negative values are floored at 500 and high values are capped at 20000.

```
### 11. Cumulative Operation - cumsum() , cumprod() , cummax() , cummin()

```py
import pandas as pd
import numpy as np

# --- Create a custom sample DataFrame ---
# This DataFrame simulates daily sales data for different products over a period.
# It's ideal for demonstrating cumulative sums, products, and grouped cumulative max/min.
data = {
    'Date': pd.to_datetime([
        '2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02', '2024-01-03',
        '2024-01-03', '2024-01-04', '2024-01-04', '2024-01-05', '2024-01-05'
    ]),
    'Product': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],
    'Daily_Sales': [100, 50, 120, 60, 90, 70, 110, 80, 130, 95],
    'Growth_Factor': [1.02, 1.01, 1.03, 1.02, 1.01, 1.03, 1.02, 1.01, 1.03, 1.02]
}
df_daily_data = pd.DataFrame(data)

# Sort by Date and Product for clear cumulative operations
df_daily_data = df_daily_data.sort_values(by=['Date', 'Product']).reset_index(drop=True)

# Display the entire sample DataFrame
print("Original Sample Daily Data DataFrame:\n", df_daily_data)
print("\n--- Cumulative Operations on Numerical Data ---")

# --- 1. df['column_name'].cumsum() ---
# Calculates the running total of a numerical column.
df_daily_data['Cumulative_Sales'] = df_daily_data['Daily_Sales'].cumsum()
print("\n1. 'Daily_Sales' with Cumulative Sum (cumsum()):\n",
      df_daily_data[['Date', 'Product', 'Daily_Sales', 'Cumulative_Sales']])

# --- 2. df['column_name'].cumprod() ---
# Calculates the running product of a numerical column.
# Useful for compounding growth rates or multipliers.
df_daily_data['Cumulative_Growth_Factor'] = df_daily_data['Growth_Factor'].cumprod()
print("\n2. 'Growth_Factor' with Cumulative Product (cumprod()):\n",
      df_daily_data[['Date', 'Product', 'Growth_Factor', 'Cumulative_Growth_Factor']])

# --- 3. df.groupby(['column_name'])['column_name'].cummax() ---
# Calculates the cumulative maximum value within each group.
# For each row in a group, it finds the maximum value seen so far in that group.
df_daily_data['Daily_Sales_CumMax_by_Product'] = \
    df_daily_data.groupby('Product')['Daily_Sales'].cummax()
print("\n3. 'Daily_Sales' with Cumulative Max (cummax()) grouped by 'Product':\n",
      df_daily_data[['Date', 'Product', 'Daily_Sales', 'Daily_Sales_CumMax_by_Product']])

# --- 4. df.groupby(['column_name'])['column_name'].cummin() ---
# Calculates the cumulative minimum value within each group.
# For each row in a group, it finds the minimum value seen so far in that group.
df_daily_data['Daily_Sales_CumMin_by_Product'] = \
    df_daily_data.groupby('Product')['Daily_Sales'].cummin()
print("\n4. 'Daily_Sales' with Cumulative Min (cummin()) grouped by 'Product':\n",
      df_daily_data[['Date', 'Product', 'Daily_Sales', 'Daily_Sales_CumMin_by_Product']])
```


![Screenshot 2025-06-14 231508](https://github.com/user-attachments/assets/4cd81e49-4b02-4636-be79-511899ec0b63)


### 12. Apply Custom function - apply() 

```py
import pandas as pd
import numpy as np

# --- Create a custom sample DataFrame ---
# This DataFrame represents sales data for different product categories
# across various months, including some missing values (NaN) for robustness.
data = {
    'Electronics': [1500, 1800, np.nan, 2200, 1900],
    'Apparel': [800, 950, 700, 1000, np.nan],
    'HomeGoods': [1200, 1100, 1300, np.nan, 1400],
    'Books': [300, 400, 350, 420, 380]
}
df_sales = pd.DataFrame(data, index=['Jan', 'Feb', 'Mar', 'Apr', 'May'])

# Display the entire sample DataFrame
print("Original Sample Sales DataFrame:\n", df_sales)
print("\n--- Custom Functions with df.apply() on Numerical Data ---")

# --- Define custom functions for demonstration ---

# Function to calculate range (max - min)
def calculate_range(series):
    """Calculates the range (max - min) of a Series, ignoring NaNs."""
    return series.max() - series.min()

# Function to categorize sum (returns multiple values)
def categorize_sum(series):
    """Categorizes the sum of a Series into 'Low', 'Medium', 'High'."""
    total_sum = series.sum()
    if total_sum < 3000:
        return 'Low'
    elif total_sum < 6000:
        return 'Medium'
    else:
        return 'High'

# Function to return multiple related statistics (for result_type='expand')
def get_stats(row_or_col):
    """Returns a Series with mean and sum, ignoring NaNs."""
    return pd.Series({
        'Mean': row_or_col.mean(),
        'Sum': row_or_col.sum()
    })

# Function that returns a Series with same length as input (for result_type='broadcast')
def normalize_and_flag_high(series):
    """
    Normalizes the series by its mean and flags values > 1.2 * mean.
    Returns a Series of 'Normalized_Value' and a 'Is_High' boolean.
    """
    mean_val = series.mean()
    if mean_val == 0: # Avoid division by zero
        return pd.Series(0, index=series.index), pd.Series(False, index=series.index)

    normalized = series / mean_val
    is_high = series > (1.2 * mean_val)
    return pd.Series(normalized, index=series.index), pd.Series(is_high, index=series.index)


# --- 1. Applying function along axis (rows or columns) ---

# a) axis=0 (default): Apply function column-wise
print("\n1a. Applying calculate_range() column-wise (axis=0, default):")
sales_range_by_category = df_sales.apply(calculate_range, axis=0)
print(sales_range_by_category)
print(f"Type of result: {type(sales_range_by_category)}") # Expected Series

# b) axis=1: Apply function row-wise
print("\n1b. Applying calculate_range() row-wise (axis=1):")
sales_range_by_month = df_sales.apply(calculate_range, axis=1)
print(sales_range_by_month)
print(f"Type of result: {type(sales_range_by_month)}") # Expected Series


# --- 2. result_type variations ---

# a) result_type='reduce' (default): Returns a Series if the function returns a scalar.
#    This is what happened in the examples above (1a and 1b).
print("\n2a. result_type='reduce' (default) for calculate_range():")
sales_range_reduced = df_sales.apply(calculate_range, axis=0, result_type='reduce')
print(sales_range_reduced)
print(f"Type of result: {type(sales_range_reduced)}") # Expected Series


# b) result_type='expand': Returns a DataFrame.
#    The applied function must return a Series (or list-like) where each element
#    becomes a new column (if axis=0) or a new row (if axis=1).
print("\n2b. result_type='expand' for get_stats():")
# Applying get_stats column-wise will create a DataFrame where rows are 'Mean' and 'Sum'
sales_stats_expanded_cols = df_sales.apply(get_stats, axis=0, result_type='expand')
print("Applying get_stats column-wise (axis=0, result_type='expand'):\n", sales_stats_expanded_cols)

# Applying get_stats row-wise will create a DataFrame where columns are 'Mean' and 'Sum'
sales_stats_expanded_rows = df_sales.apply(get_stats, axis=1, result_type='expand')
print("\nApplying get_stats row-wise (axis=1, result_type='expand'):\n", sales_stats_expanded_rows)


# c) result_type='broadcast': Returns a DataFrame with the same shape as the original.
#    The applied function must return a Series with the same index/columns as the input Series.
print("\n2c. result_type='broadcast' for normalize_and_flag_high():")
# This function returns two Series (normalized_values, is_high_flag)
# When apply with broadcast, it's generally expected the function returns a single Series
# that is broadcastable. If it returns multiple, it might behave differently or raise error
# based on context. For a simpler broadcast demo, let's just return one Series.

def normalize_series(series):
    """Normalizes a Series by its mean."""
    mean_val = series.mean()
    return series / mean_val if mean_val != 0 else pd.Series(0, index=series.index)

# Apply normalize_series column-wise to broadcast back to the original shape
df_normalized_sales = df_sales.apply(normalize_series, axis=0, result_type='broadcast')
print("Applying normalize_series column-wise (axis=0, result_type='broadcast'):\n", df_normalized_sales)

# Apply normalize_series row-wise to broadcast back to the original shape
df_normalized_sales_row = df_sales.apply(normalize_series, axis=1, result_type='broadcast')
print("\nApplying normalize_series row-wise (axis=1, result_type='broadcast'):\n", df_normalized_sales_row)


# Important Notes:
# - .apply() is flexible but can be slower than vectorized operations for simple tasks.
# - result_type='broadcast' is less common for returning new values but useful when
#   you want to apply a transformation that keeps the original shape.
# - For 'broadcast', the function must produce a Series of the same size as the input chunk.


```

### 13. Apply Custom function - applymap() 

```py
import pandas as pd
import numpy as np

# --- Create a custom sample DataFrame ---
# This DataFrame represents hypothetical scores or values for various subjects/metrics,
# suitable for element-wise operations with applymap().
data = {
    'Math_Score': [85.5, 92.1, 78.9, 95.0, 88.3, np.nan, 70.2],
    'Physics_Score': [70.3, 88.7, 95.2, 75.8, np.nan, 80.1, 72.5],
    'Chemistry_Score': [90.8, 85.0, 70.1, np.nan, 92.5, 78.4, 83.9],
    'Biology_Score': [75.0, np.nan, 88.2, 91.5, 80.0, 76.8, 82.3]
}
df_scores = pd.DataFrame(data, index=['Student A', 'Student B', 'Student C',
                                      'Student D', 'Student E', 'Student F', 'Student G'])

# Display the entire sample DataFrame
print("Original Sample Scores DataFrame:\n", df_scores)
print("\n--- Custom Functions with df.applymap() on Numerical Data ---")

# --- Define custom functions for demonstration ---

# Function to round each score to the nearest integer
def round_score(score):
    """Rounds a numerical score to the nearest integer, handling NaNs."""
    if pd.isna(score):
        return np.nan
    return round(score)

# Function to convert score to a percentage string
def score_to_percentage(score):
    """Converts a numerical score to a formatted percentage string."""
    if pd.isna(score):
        return np.nan # Or any other representation for missing data
    return f"{score:.1f}%"

# Function to categorize each individual score as 'Pass' or 'Fail'
def pass_fail_threshold(score, threshold=75.0):
    """Categorizes a score as 'Pass' or 'Fail' based on a threshold."""
    if pd.isna(score):
        return np.nan
    return 'Pass' if score >= threshold else 'Fail'


# --- Applying df.applymap() ---

print("\n1. Applying round_score() to round all scores:")
df_rounded_scores = df_scores.applymap(round_score)
print(df_rounded_scores)

print("\n2. Applying score_to_percentage() to format all scores as percentages:")
df_percentage_scores = df_scores.applymap(score_to_percentage)
print(df_percentage_scores)

print("\n3. Applying pass_fail_threshold() to categorize each score individually (threshold=75.0):")
df_pass_fail = df_scores.applymap(lambda x: pass_fail_threshold(x, threshold=75.0))
print(df_pass_fail)

# Important Note:
# - `df.applymap()` is strictly element-wise. It takes a function that operates on a single scalar value
#   and applies it to every element in the DataFrame.
# - It is generally slower than vectorized Pandas operations or NumPy ufuncs for common tasks,
#   but it's invaluable when you need to perform a custom operation on each individual cell that
#   doesn't easily vectorize.
# - For column-wise or row-wise operations that might return multiple values (e.g., mean, sum),
#   `df.apply()` is the appropriate method.


```

### 14. Apply Custom function - map() 

```py
import pandas as pd
import numpy as np

# --- Create a custom sample DataFrame ---
# This DataFrame simulates sales data for different items, including quantities and unit prices.
# It's suitable for demonstrating element-wise transformations on a single Series (column).
data = {
    'Item_ID': ['A101', 'A102', 'A103', 'A104', 'A105', 'A106', 'A107'],
    'Quantity_Sold': [10, 15, 8, 20, 12, 5, 18],
    'Unit_Price_USD': [25.50, 10.00, 50.75, 12.25, 30.00, 100.00, 8.50],
    'Discount_Rate': [0.05, 0.10, 0.0, 0.08, 0.05, 0.15, 0.0] # Some items have no discount
}
df_sales = pd.DataFrame(data)

# Display the entire sample DataFrame
print("Original Sample Sales DataFrame:\n", df_sales)
print("\n--- Custom Functions with Series.map() on Numerical Data ---")

# --- Define custom functions for demonstration ---

# Function to calculate final price after discount for a single unit price
def calculate_net_price(price, discount_rate):
    """Calculates the net price after applying a discount."""
    return price * (1 - discount_rate)

# Function to categorize unit price into 'Low', 'Medium', 'High'
def categorize_price(price):
    """Categorizes a unit price based on predefined thresholds."""
    if price < 20:
        return 'Low'
    elif price < 60:
        return 'Medium'
    else:
        return 'High'

# Function to apply a markup based on the original value
def apply_markup(price):
    """Applies a markup to a price based on its value."""
    if price > 50:
        return price * 1.10  # 10% markup for high-priced items
    else:
        return price * 1.05  # 5% markup for low-priced items


# --- Applying Series.map() ---

# 1. Applying a simple function to 'Quantity_Sold'
# Let's say we want to convert quantities to 'dozen' units.
def to_dozen(quantity):
    return quantity / 12

df_sales['Quantity_in_Dozens'] = df_sales['Quantity_Sold'].map(to_dozen)
print("\n1. 'Quantity_Sold' converted to 'Quantity_in_Dozens':")
print(df_sales[['Quantity_Sold', 'Quantity_in_Dozens']])

# 2. Applying a more complex categorization function to 'Unit_Price_USD'
df_sales['Price_Category'] = df_sales['Unit_Price_USD'].map(categorize_price)
print("\n2. 'Unit_Price_USD' categorized into 'Price_Category':")
print(df_sales[['Unit_Price_USD', 'Price_Category']])

# 3. Applying apply_markup() to 'Unit_Price_USD'
df_sales['Unit_Price_with_Markup'] = df_sales['Unit_Price_USD'].map(apply_markup)
print("\n3. 'Unit_Price_USD' with 'Unit_Price_with_Markup':")
print(df_sales[['Unit_Price_USD', 'Unit_Price_with_Markup']])

# Important Note:
# - `Series.map()` is designed for element-wise transformations on a **Series** (single column).
# - It takes a dictionary (for mapping values) or a function (for applying a transformation).
# - If you need to pass multiple arguments to your custom function (like 'price' and 'discount_rate' for 'calculate_net_price'),
#   you cannot directly use `.map()` with a simple function call. For such cases, `df.apply()` with `axis=1`
#   (to iterate row by row) or vectorized operations are more appropriate.
# - For example, to calculate `Net_Price` needing both `Unit_Price_USD` and `Discount_Rate`, you'd use `apply`:
# df_sales['Net_Price_per_Unit'] = df_sales.apply(lambda row: calculate_net_price(row['Unit_Price_USD'], row['Discount_Rate']), axis=1)
# print("\nCalculated 'Net_Price_per_Unit' using apply (needs multiple columns):\n",
#       df_sales[['Unit_Price_USD', 'Discount_Rate', 'Net_Price_per_Unit']])

```

### 15. Apply Custom function - lambda function

```py
import pandas as pd
import numpy as np

# --- Create a custom sample DataFrame ---
# This DataFrame simulates quarterly performance metrics for different regions,
# including some missing values (NaN) to show robustness.
data = {
    'Quarter': ['Q1', 'Q2', 'Q3', 'Q4', 'Q1', 'Q2', 'Q3', 'Q4'],
    'Region': ['East', 'East', 'East', 'East', 'West', 'West', 'West', 'West'],
    'Sales_Revenue': [150000, 180000, 165000, 200000, 120000, np.nan, 140000, 170000],
    'Marketing_Spend': [15000, 18000, 17000, 21000, 12000, 13000, 14500, 18000],
    'Profit_Margin_Pct': [0.15, 0.18, 0.16, 0.20, 0.12, 0.10, 0.14, 0.17]
}
df_performance = pd.DataFrame(data)

# Display the entire sample DataFrame
print("Original Sample Performance DataFrame:\n", df_performance)
print("\n--- Pandas Operations with Lambda Functions on Numerical Data ---")

# --- 1. Applying a simple lambda function to a single column ---
# Convert 'Sales_Revenue' from USD to a custom currency (e.g., divide by 1000 for 'K USD')
df_performance['Sales_K_USD'] = df_performance['Sales_Revenue'].apply(lambda x: x / 1000 if pd.notna(x) else x)
print("\n1. 'Sales_Revenue' converted to 'Sales_K_USD' using lambda (element-wise):\n",
      df_performance[['Sales_Revenue', 'Sales_K_USD']])

# --- 2. Applying a lambda function with a conditional logic ---
# Categorize 'Profit_Margin_Pct' into 'High' or 'Low'
df_performance['Profit_Category'] = df_performance['Profit_Margin_Pct'].apply(
    lambda x: 'High Profit' if x > 0.15 else ('Low Profit' if pd.notna(x) else np.nan)
)
print("\n2. 'Profit_Margin_Pct' categorized using lambda with if/else:\n",
      df_performance[['Profit_Margin_Pct', 'Profit_Category']])

# --- 3. Applying a lambda function across rows (axis=1) ---
# Calculate 'Return_on_Marketing_Spend' for each row.
# This requires accessing multiple columns within the lambda.
df_performance['ROM_Ratio'] = df_performance.apply(
    lambda row: (row['Sales_Revenue'] - row['Marketing_Spend']) / row['Marketing_Spend']
    if row['Marketing_Spend'] > 0 and pd.notna(row['Sales_Revenue'])
    else np.nan,
    axis=1
)
print("\n3. 'ROM_Ratio' calculated across rows using lambda (axis=1):\n",
      df_performance[['Sales_Revenue', 'Marketing_Spend', 'ROM_Ratio']])

# --- 4. Applying a lambda function with grouping ---
# Calculate the percentage of total sales within each region for each entry.
# This requires a two-step process: first sum by group, then apply lambda for percentage.
total_sales_by_region = df_performance.groupby('Region')['Sales_Revenue'].transform('sum')

df_performance['Sales_Share_of_Region'] = df_performance.apply(
    lambda row: row['Sales_Revenue'] / total_sales_by_region[row.name]
    if pd.notna(row['Sales_Revenue']) and total_sales_by_region[row.name] > 0
    else np.nan,
    axis=1
)
print("\n4. 'Sales_Share_of_Region' using lambda with grouping context (complex example):\n",
      df_performance[['Region', 'Sales_Revenue', 'Sales_Share_of_Region']])

```
